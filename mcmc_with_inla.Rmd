---
title: "Markov chain Monte Carlo with the Integrated Nested Laplace Approximation"
author: "Martin Outzen Berild"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: 
  html_document:
    toc: true
    toc_depth: 1
    toc_float: true
---

\newcommand{\vect}[1]{\mathbf{#1}} 
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\nats}{\mathbb{N}}
\newcommand{\trsp}[1]{{#1}^\intercal}
\newcommand{\ssep}{\,;\,}
\newcommand{\st}{\,:\,}
\newcommand{\ind}{\mathbbm{1}} 
\newcommand{\argmax}{\text{argmax}}

\newcommand{\prob}[1]{\mathrm{p}#1}
\newcommand{\norm}{\mathcal{N}} 
\newcommand{\pois}{\mathrm{Pois}}
\newcommand{\ga}{\mathrm{Gamma}}
\newcommand{\bin}{\mathrm{binom}}
\newcommand{\E}{{\rm I\kern-.3em E}}
\newcommand{\Var}{\mathrm{Var}} 
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Corr}{\mathrm{Corr}}
\newcommand{\given}[1]{\, | \, #1}
\newcommand{\lik}{\mathcal{L}}



# `r icon:: ii("arrow-right-b")`  Bayesian Lasso

## `r icon:: ii("arrow-right-b")``r icon:: ii("arrow-right-b")`  Theory
Lasso regression performs both coefficient estimates and feature selection at the same time. The estimation is done by the equation

$$
\mathrm{minimize} \sum\limits_{i=1}^n\left(y_i - \beta_0 + \sum_{j=1}^p\beta_jx_{ij}\right)^2 + \lambda \sum\limits_{j=1}^p|\beta_j|,
$$
where $y_i$ is the response and $x_ij$. There are $n$ observations and $p$ covariates. The penality term $\lambda$ is found with the minimization and cross-validation and is set to be $\lambda = 0.73$. The coefficients have Laplace priors, which is defined as
$$
\pi(\beta) = \frac{1}{2\sigma} \mathrm{exp}\left(-\frac{|\beta - \mu|}{\sigma} \right), x \in \reals.
$$

$\mu = 0$ and $\sigma = 1/\tau = 1/0.73$ are location and scale, and are assumed to be independent a priori s.t. the prior is given by the product of five Laplace distributions.  

The response will be modelled by 
$$ 
\vect{y} = \matr{X}\vect{\beta} + \vect{\epsilon},\enspace \vect{\epsilon} \sim \norm_n(\vect{0},\frac{1}{\tau}\matr{I_n}),
$$
which implies that $\vect{y} \sim \norm_n(\matr{x}\vect{\beta},\frac{1}{\tau}\matr{I_n}))$. We will also assume that $\vect{\beta}$ and $\tau$ are independent a prior, meaning $\pi(\vect{\beta},\tau) = \pi(\vect{\beta})\pi(\tau)$. 
Further the coefficient $\vect{\beta}$ will have a Gaussian proposal distribution given by 

$$
\vect{\beta}^* \sim q(\beta^*\given\beta, \vect{y}, \tau) = \norm_p\left(\vect{0},(4\cdot\matr{X}^T\matr{X})^{-1}\right),
$$

<p style="color:red;background-color:yellow;">This might be wrong! mean of previous $\beta$</p>
and the prior for $\tau$ is given by
$$
\tau \sim \ga\left(1, 5\mathrm{e}-0.5\right).
$$


## `r icon:: ii("arrow-right-b")``r icon:: ii("arrow-right-b")`  Method

### `r icon:: ii("arrow-right-b")``r icon:: ii("arrow-right-b")``r icon:: ii("arrow-right-b")`  McMC

$$ 
\pi(\beta,\tau \given\vect{y}) \propto \pi(\vect{y}|\beta,\tau)\pi(\beta)\pi(\tau)
$$



$$ 
\pi(\beta\given\vect{y},\tau) \propto \pi(\vect{y}|\beta,\tau)\pi(\beta)
$$


$$
\begin{align}
\pi(\tau\given\vect{y},\beta) & \propto  \pi(\vect{y}|\beta,\tau)\pi(\tau) \\
 & \propto  \tau^{\phi - 1}\mathrm{e}^{-\tau/\theta}\tau^{1/2}\exp\left\{\frac{1}{2}(\vect{y}-\matr{X}\beta)^T\tau\matr{I}_n(\vect{y}-\matr{X}\beta)\right\} \\ 
 & = \tau^{\phi + \frac{1}{2} - 1} \exp\left\{-\tau(\frac{1}{\theta} + \frac{1}{2}(\vect{y}-\matr{X}\beta)^T(\vect{y}-\matr{X}\beta)) \right\} \\
 & \propto \ga\left\{\phi + \frac{1}{2}, \left(\frac{1}{\theta} + \frac{1}{2}(\vect{y}-\matr{X}\beta)^T(\vect{y}-\matr{X}\beta)\right) ^{-1} \right\}
\end{align}
$$

From the markov assumption and the Metropolis choice the acceptance probability is given by

$$
\mathrm{acc.prob} = \min\left\{1, \frac{\pi(\vect{y}\given\beta^*,\tau)\pi(\beta^*)q(\beta\given\beta^*)}{\pi(\vect{y}\given\beta,\tau)\pi(\beta)q(\beta^*\given\beta)}\right\}
$$


### `r icon:: ii("arrow-right-b")``r icon:: ii("arrow-right-b")``r icon:: ii("arrow-right-b")`  McMC with INLA
The Laplace prior is not available in **R-INLA**, but by fixating the values of $\beta$, and conditioning on them, the model can be fitted. This means that we draw from the same proposal distribution to get samples of $\beta$. Then we fit the model with **R-INLA**, to get the conditional marginal likelihood $\pi(\vect{y}\given\beta)$, and the approximation of the conditional marginal distribution $\pi(\tau\given\vect{y},\beta)$. The posterior marginal $\pi(\tau\given\vect{y})$ is then found by Bayesian model averaging. The acceptance probability is now given by the equation
$$
\alpha = \min\left\{1,\frac{\pi(\vect{y}\given\beta^*)\pi(\beta^*)q(\beta\given\beta^*)}{\vect{y}\given\beta)\pi(\beta)q(\beta^*\given\beta)}\right\}
$$

## `r icon:: ii("arrow-right-b")``r icon:: ii("arrow-right-b")`  Results

# `r icon:: ii("arrow-right-b")`  Missing Covariates

# `r icon:: ii("arrow-right-b")`  Spatial Econometric Model

# `r icon:: ii("arrow-right-b")`  Classification










<style>
html, body{
overflow-x: hidden;
}
.fluid-row{
  /* also subtract section padding (1.5rem) */
  margin: 0 -601.5rem;
  /* add back section padding (1.5rem) */
  padding: 0.25rem 1.5rem;
  background: rgb(54, 127, 209);
  /* border has to be solid, not RGBa */
  /* 9600px or equiv (600rem = 9600/16) */
  border-left: 600rem solid rgb(54, 127, 209);
  border-right: 600rem solid rgb(54, 127, 209);
  z-index: -1;
}
#TOC{
z-index: 100;
}
.title{
font-size: 25px;
color: white;
}
.author{
font-size: 16px;
color: white;
}

.date{
font-size: 16px;
color: white;
}
h1{
font-size: 25px;
font-weight: bold;
}
h2{
font-size: 20px;
font-weight: bold;
}
h3{
font-size: 18px;
font-weight: bold;
}
</style>